{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c62c615-7555-4d80-8cd0-b1dc40432d8e",
   "metadata": {},
   "source": [
    "# Graded: x of 8 correct\n",
    "- [x] Read the text file\n",
    "- [x] Remove punctuation\n",
    "- [x] Convert to lowercase\n",
    "- [x] Split into words\n",
    "- [x] Create a `set` of unique words\n",
    "- [x] Dictionary of word counts\n",
    "- [x] Display unique words and frequencies\n",
    "- [x] Code organization and comments\n",
    "\n",
    "Comments: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9b181-ebe4-49aa-ae71-0bd72176b056",
   "metadata": {},
   "source": [
    "## Unit 2 Programming Assignment\n",
    "* The objective of this assignment is for you to write code that reads a text file and computes on the text data.\n",
    "* Your program should read a text file, do some cleaning of the text and compute word statistics.\n",
    "* In particular, the code should:\n",
    "    1. Read the content of the text file `Unit2_Python_learning_journey.txt`.\n",
    "        * The text in this file is from the free Python text book, [Python for everybody](https://www.py4e.com/book.php)\n",
    "    2. Remove punctuation and convert all words to lowercase.\n",
    "    3. Split the text into individual words.\n",
    "    4. Use a set to find all unique words.\n",
    "    5. Use a dictionary to count the frequency of each unique word.\n",
    "    6. Display each unique word along with its frequency.\n",
    "* You should organize your code appropriately to show a clean and thoughtful design.\n",
    "    * Use functions as needed.\n",
    "    * Break up into cells so smaller pieces can be easily tested.\n",
    "    * Add teh appropriate documentation to make your code comprehensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79112d3d-c6af-405b-b8f4-cc54aeedfa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as: 2\n",
      "you: 20\n",
      "progress: 2\n",
      "through: 1\n",
      "the: 15\n",
      "rest: 1\n",
      "of: 5\n",
      "book: 3\n",
      "donâ€™t: 3\n",
      "be: 3\n",
      "afraid: 1\n",
      "if: 5\n",
      "concepts: 2\n",
      "seem: 1\n",
      "to: 18\n",
      "fit: 1\n",
      "together: 1\n",
      "well: 1\n",
      "first: 3\n",
      "time: 4\n",
      "when: 2\n",
      "were: 1\n",
      "learning: 3\n",
      "speak: 1\n",
      "it: 12\n",
      "was: 3\n",
      "not: 1\n",
      "a: 17\n",
      "problem: 2\n",
      "for: 2\n",
      "your: 4\n",
      "few: 4\n",
      "years: 3\n",
      "that: 9\n",
      "just: 1\n",
      "made: 1\n",
      "cute: 1\n",
      "gurgling: 1\n",
      "noises: 1\n",
      "and: 17\n",
      "ok: 1\n",
      "took: 3\n",
      "six: 1\n",
      "months: 1\n",
      "move: 2\n",
      "from: 3\n",
      "simple: 2\n",
      "vocabulary: 1\n",
      "sentences: 2\n",
      "56: 1\n",
      "more: 4\n",
      "paragraphs: 1\n",
      "able: 1\n",
      "write: 1\n",
      "an: 1\n",
      "interesting: 1\n",
      "complete: 1\n",
      "short: 1\n",
      "story: 1\n",
      "on: 1\n",
      "own: 1\n",
      "we: 4\n",
      "want: 1\n",
      "learn: 2\n",
      "python: 1\n",
      "much: 1\n",
      "rapidly: 1\n",
      "so: 1\n",
      "teach: 1\n",
      "all: 3\n",
      "at: 4\n",
      "same: 1\n",
      "over: 1\n",
      "next: 1\n",
      "chapters: 1\n",
      "but: 1\n",
      "is: 3\n",
      "like: 1\n",
      "new: 1\n",
      "language: 2\n",
      "takes: 1\n",
      "absorb: 2\n",
      "understand: 1\n",
      "before: 1\n",
      "feels: 1\n",
      "natural: 1\n",
      "leads: 1\n",
      "some: 2\n",
      "confusion: 1\n",
      "visit: 1\n",
      "revisit: 1\n",
      "topics: 1\n",
      "try: 1\n",
      "get: 2\n",
      "see: 3\n",
      "big: 2\n",
      "picture: 2\n",
      "while: 2\n",
      "are: 7\n",
      "defining: 1\n",
      "tiny: 1\n",
      "fragments: 1\n",
      "make: 1\n",
      "up: 3\n",
      "written: 1\n",
      "linearly: 1\n",
      "taking: 1\n",
      "course: 1\n",
      "will: 3\n",
      "in: 4\n",
      "linear: 1\n",
      "fashion: 1\n",
      "hesitate: 1\n",
      "very: 1\n",
      "nonlinear: 1\n",
      "how: 1\n",
      "approach: 1\n",
      "material: 5\n",
      "look: 3\n",
      "forwards: 1\n",
      "backwards: 1\n",
      "read: 1\n",
      "with: 4\n",
      "light: 1\n",
      "touch: 1\n",
      "by: 2\n",
      "skimming: 1\n",
      "advanced: 1\n",
      "without: 1\n",
      "fully: 1\n",
      "understanding: 2\n",
      "details: 1\n",
      "can: 2\n",
      "better: 1\n",
      "why: 1\n",
      "programming: 3\n",
      "reviewing: 1\n",
      "previous: 1\n",
      "even: 2\n",
      "redoing: 1\n",
      "earlier: 1\n",
      "exercises: 1\n",
      "realize: 1\n",
      "actually: 1\n",
      "learned: 1\n",
      "lot: 1\n",
      "currently: 1\n",
      "staring: 2\n",
      "seems: 2\n",
      "bit: 2\n",
      "impenetrable: 1\n",
      "usually: 2\n",
      "there: 2\n",
      "wonderful: 1\n",
      "ah: 1\n",
      "hah: 1\n",
      "moments: 1\n",
      "where: 1\n",
      "pounding: 1\n",
      "away: 2\n",
      "rock: 1\n",
      "hammer: 1\n",
      "chisel: 1\n",
      "step: 1\n",
      "indeed: 1\n",
      "building: 1\n",
      "beautiful: 1\n",
      "sculpture: 1\n",
      "something: 1\n",
      "particularly: 1\n",
      "hard: 1\n",
      "no: 1\n",
      "value: 1\n",
      "staying: 1\n",
      "night: 1\n",
      "take: 2\n",
      "break: 1\n",
      "nap: 1\n",
      "have: 1\n",
      "snack: 1\n",
      "explain: 1\n",
      "what: 1\n",
      "having: 1\n",
      "someone: 1\n",
      "or: 1\n",
      "perhaps: 1\n",
      "dog: 1\n",
      "then: 1\n",
      "come: 1\n",
      "back: 2\n",
      "fresh: 1\n",
      "eyes: 1\n",
      "i: 1\n",
      "assure: 1\n",
      "once: 1\n",
      "really: 1\n",
      "easy: 1\n",
      "elegant: 1\n",
      "simply: 1\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import string\n",
    "\n",
    "def read_text_file(path):\n",
    "    \"\"\"\n",
    "    Reads the content of the file specified by path.\n",
    "    \n",
    "    Parameters:\n",
    "        path (str): The path to the text file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The content of the file.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def text_cleaning(text):\n",
    "    \"\"\"\n",
    "    Sanitizes the text by removing punctuation and converting to lowercase.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The raw text to be sanitized.\n",
    "        \n",
    "    Returns:\n",
    "        str: The sanitized text.\n",
    "    \"\"\"\n",
    "    # Remove punctuation using str.translate and str.maketrans\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    sanitized_text = text.translate(translator)\n",
    "    # Convert to lowercase\n",
    "    sanitized_text = sanitized_text.lower()\n",
    "    return sanitized_text\n",
    "\n",
    "def split_into_words(text):\n",
    "    \"\"\"\n",
    "    Splits the sanitized text into individual words.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The sanitized text.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of words.\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def tally_word_frequencies(words):\n",
    "    \"\"\"\n",
    "    Tallies the frequency of each word in the list.\n",
    "    \n",
    "    Parameters:\n",
    "        words (list): A list of words.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with words as keys and their frequencies as values.\n",
    "    \"\"\"\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    return word_freq\n",
    "\n",
    "def show_word_frequencies(word_freq):\n",
    "    \"\"\"\n",
    "    Displays the frequency of each word in the dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "        word_freq (dict): A dictionary with words and their frequencies.\n",
    "    \"\"\"\n",
    "    for word, freq in word_freq.items():\n",
    "        print(f\"{word}: {freq}\")\n",
    "\n",
    "# Main function to process the text file and compute word statistics\n",
    "def process_text_file(path):\n",
    "    \"\"\"\n",
    "    Reads a text file, sanitizes the text, and computes word statistics.\n",
    "    \n",
    "    Parameters:\n",
    "        path (str): The path to the text file.\n",
    "    \"\"\"\n",
    "    # Step 1: Read the content of the file\n",
    "    text_content = read_text_file(path)\n",
    "    \n",
    "    # Step 2: Sanitize the text\n",
    "    sanitized_text = text_cleaning(text_content)\n",
    "    \n",
    "    # Step 3: Split the text into individual words\n",
    "    word_list = split_into_words(sanitized_text)\n",
    "    \n",
    "    # Step 4: Tally the frequency of each unique word\n",
    "    word_frequencies = tally_word_frequencies(word_list)\n",
    "    \n",
    "    # Step 5: Display each unique word along with its frequency\n",
    "    show_word_frequencies(word_frequencies)\n",
    "\n",
    "# File path to the text file\n",
    "file_path = r'C:\\Users\\Nguyen\\Downloads\\Unit2_Python_learning_journey.txt'\n",
    "\n",
    "# Run the main function\n",
    "process_text_file(file_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
